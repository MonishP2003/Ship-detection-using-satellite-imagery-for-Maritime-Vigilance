{"cells":[{"cell_type":"markdown","metadata":{},"source":["Importing libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cv2\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","import seaborn as sns\n","import numpy as np\n","from skimage.util import montage\n","from skimage.segmentation import mark_boundaries\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_image_dir = r\"D:\\Ship detection dataset\\train_v2\"\n","train_encode_file = r\"D:\\Ship detection dataset\\train_ship_segmentations_v2.csv\"\n","test_image_dir = r\"D:\\Ship detection dataset\\test_v2\""]},{"cell_type":"markdown","metadata":{},"source":["Train directory"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_images = os.listdir(train_image_dir)\n","print(f\"Total {len(train_images)} images in the train directory. \\nHere are the first 5 images: - {train_images[:5]}\")"]},{"cell_type":"markdown","metadata":{},"source":["Test directory"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_images = os.listdir(test_image_dir)\n","print(f\"Total {len(test_images)} images in the test directory. \\nHere are the first 5 images: - {test_images[:5]}\")"]},{"cell_type":"markdown","metadata":{},"source":["Visualize some test images"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(15, 15))\n","for i in range(16):\n","    plt.subplot(4, 4, i + 1)\n","    plt.imshow(cv2.imread(test_image_dir + '/' + test_images[i]))\n","    plt.title(f\"{test_images[i]}\", weight='bold')\n","    plt.axis('off')\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{},"source":["Read the CSV file"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv(train_encode_file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.info()  # Some information about train_encode_file"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[df[\"ImageId\"] == \"00021ddc3.jpg\"]  # In df, for the same image, we have more than one ship, and each ship is in a separate row."]},{"cell_type":"markdown","metadata":{},"source":["Build a dictionary where the key is the image id, and the value is the number of ships in the image"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ship_num = {}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for index, row in df.iterrows():\n","    image_id = row['ImageId']\n","    encoding = row['EncodedPixels']\n","    if pd.isna(encoding) or encoding == \"\":\n","        ship_num[image_id] = 0\n","    else:\n","        if image_id in ship_num:\n","            ship_num[image_id] += 1\n","        else:\n","            ship_num[image_id] = 1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(ship_num)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["img = cv2.imread(train_image_dir + \"/30d3f7721.jpg\")\n","plt.imshow(img)\n","print(f\"Number of ships in this image is: {ship_num['30d3f7721.jpg']}\")  # Verify the number of ships for the image with id 30d3f7721.jpg"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(len(ship_num))\n","# Coincides with the unique number of elements\n","# Number of images"]},{"cell_type":"markdown","metadata":{},"source":["Transform the dictionary into a DataFrame for easier information manipulation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MonData = pd.DataFrame(list(ship_num.items()), columns=['ImageId', 'ships'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MonData.head(10)"]},{"cell_type":"markdown","metadata":{},"source":["We verified that the image with id 00003e153.jpg does not have any ships"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["img = cv2.imread(train_image_dir + \"/00003e153.jpg\")\n","plt.imshow(img)\n","print(\"This image with id \\\"00003e153.jpg\\\" does not represent any ship according to the previous table.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(12, 8))  # Adjust the figure size\n","sns.countplot(data=MonData, x='ships', palette='Set2')\n","# Add labels and a title\n","plt.title('Distribution of the number of images', fontsize=16)\n","plt.xlabel('Number of ships', fontsize=14)\n","plt.ylabel('Number of occurrences', fontsize=14)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MonData['ships'].value_counts()  # Number of images with the same number of ships\n","# We can observe an imbalance in the distribution of images based on the number of ships."]},{"cell_type":"markdown","metadata":{},"source":["List of image paths"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["image_paths = [\n","    os.path.join(train_image_dir, filename) for filename in os.listdir(train_image_dir)\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["image_paths[10:20]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(df.isnull().sum())  # Check for missing values"]},{"cell_type":"markdown","metadata":{},"source":["Detailed analysis of images"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(5):\n","    image = cv2.imread(image_paths[i])\n","    height, width, channels = image.shape  # Get the dimensions of the image\n","    image_size = height * width  # Calculate the size of the image\n","    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n","    plt.title(f\"Image {i + 1}\")\n","    plt.axis('off')\n","    plt.show()\n","    print(f\"Image dimensions: {height} x {width} pixels\")\n","    print(f\"Number of channels: {channels}\")\n","    print(f\"Image size: {image_size} pixels\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('Number of images in CSV:', len(df))\n","print('Number of images in the image file:', len(image_paths))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["number_images_without_ship = 0\n","number_images_with_ship = 0"]},{"cell_type":"markdown","metadata":{},"source":["Iterate through the ship_num dictionary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for number_ships in ship_num.values():\n","    if number_ships == 0:\n","        number_images_without_ship += 1\n","    else:\n","        number_images_with_ship += 1"]},{"cell_type":"markdown","metadata":{},"source":["Create two lists for the x and y axes of the graph"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["categories = ['Without Ship', 'With Ship']\n","number_images = [number_images_without_ship, number_images_with_ship]\n","print(\"Number of images without ship\", number_images_without_ship)\n","print(\"Number of images with ship\", number_images_with_ship)\n","# Create a bar chart\n","plt.bar(categories, number_images)"]},{"cell_type":"markdown","metadata":{},"source":["Label the axes of the graph"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.xlabel('Category')\n","plt.ylabel('Number of images')"]},{"cell_type":"markdown","metadata":{},"source":["Display the graph"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Create a new column 'Size' that contains the number of pixels per ship"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['Size'] = df['EncodedPixels'].apply(lambda x: 0 if pd.isna(x) else sum(map(int, str(x).split()[1::2])))"]},{"cell_type":"markdown","metadata":{},"source":["Group the data by size and count the number of ships of each size"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ship_sizes = df[df['Size'] > 0]['Size'].value_counts().reset_index()\n","ship_sizes.columns = ['Ship Size (in pixels)', 'Number of Ships']"]},{"cell_type":"markdown","metadata":{},"source":["Sort the data by size in ascending order"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ship_sizes = ship_sizes.sort_values(by='Ship Size (in pixels)')"]},{"cell_type":"markdown","metadata":{},"source":["Plot the graph"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(12, 6))\n","plt.hist(ship_sizes['Ship Size (in pixels)'], bins=50, color='skyblue', edgecolor='black')\n","#plt.bar(ship_sizes['Ship Size (in pixels)'], ship_sizes['Number of Ships'])\n","plt.title('Number of Ships by Size')\n","plt.xlabel('Ship Size (in pixels)')\n","plt.ylabel('Number of Ships')\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['file_size_kb'] = df['ImageId'].map(\n","    lambda c_img_id: os.stat(os.path.join(train_image_dir, c_img_id)).st_size / 1024)  # calculate file sizes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_color_histogram(image_path):\n","    image = cv2.imread(image_path)\n","    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert the image from BGR to RGB (Matplotlib uses the RGB format)\n","    plt.imshow(image_rgb)\n","    plt.title('Original Image')\n","    plt.axis('off')\n","    plt.show()\n","    r, g, b = cv2.split(image_rgb)  # Split the image into its color channels (Red, Green, Blue)\n","    plt.figure(figsize=(12, 4))\n","    # Histogram for the red channel\n","    plt.subplot(131)\n","    plt.hist(r.flatten(), bins=256, color='red', alpha=0.7, rwidth=0.8)\n","    plt.title('Red Histogram')\n","    plt.xlabel('Pixel Value')\n","    plt.ylabel('Frequency')\n","\n","    # Histogram for the green channel\n","    plt.subplot(132)\n","    plt.hist(g.flatten(), bins=256, color='green', alpha=0.7, rwidth=0.8)\n","    plt.title('Green Histogram')\n","    plt.xlabel('Pixel Value')\n","    plt.ylabel('Frequency')\n","\n","    # Histogram for the blue channel\n","    plt.subplot(133)\n","    plt.hist(b.flatten(), bins=256, color='blue', alpha=0.7, rwidth=0.8)\n","    plt.title('Blue Histogram')\n","    plt.xlabel('Pixel Value')\n","    plt.ylabel('Frequency')\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["image_paths[0:]\n","plot_color_histogram(image_paths[0])\n","plot_color_histogram(image_paths[10])\n","plot_color_histogram(image_paths[900])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def orientation_distribution(image_path):\n","    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Load the image in grayscale\n","    # Apply the Sobel filter to get the gradients\n","    sobelx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=5)\n","    sobely = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5)\n","    gradient_orientation = np.arctan2(sobely, sobelx)  # Calculate the orientation of the gradients\n","    gradient_orientation_degrees = np.degrees(gradient_orientation)  # Convert the orientation to degrees\n","    flattened_orientation = gradient_orientation_degrees.flatten()  # Flatten the array for the histogram\n","    # Show the original image\n","    plt.subplot(121)\n","    plt.imshow(image, cmap='gray')\n","    plt.title('Original Image')\n","    plt.axis('off')\n","    # Show the histogram of orientations\n","    plt.subplot(122)\n","    plt.hist(flattened_orientation, bins=36, range=[-180, 180], color='black', alpha=0.7)\n","    plt.title('Orientation Histogram')\n","    plt.xlabel('Orientation in degrees')\n","    plt.ylabel('Frequency')\n","    plt.tight_layout()  # Automatically adjust spaces to avoid overlap\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["orientation_distribution(image_paths[10])"]},{"cell_type":"markdown","metadata":{},"source":["\n","<br>\n","**The maximum frequency of the diagram is 17500, corresponding to an orientation of 100 degrees. This suggests that the dominant orientation of objects in the image is 100 degrees, which corresponds to the orientation of the ship.**<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(1, 5):\n","    orientation_distribution(image_paths[i])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def detect_keypoints(image_path):\n","    image = cv2.imread(image_path)\n","    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    orb = cv2.ORB_create()  # Initialize the ORB extractor\n","    keypoints, descriptors = orb.detectAndCompute(gray_image, None)  # Detect keypoints and descriptors with ORB\n","    image_with_keypoints = cv2.drawKeypoints(image, keypoints, None)  # Draw keypoints on the image\n","    # Display the image with keypoints\n","    plt.imshow(cv2.cvtColor(image_with_keypoints, cv2.COLOR_BGR2RGB))\n","    plt.title('Keypoint Detection with ORB (Oriented FAST and Rotated BRIEF)')\n","    plt.axis('off')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Call the detect_keypoints function for a specific image path"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["detect_keypoints(image_paths[10])"]},{"cell_type":"markdown","metadata":{},"source":["\n","<br>\n","We notice that most of the keypoints obtained by the ORB filter are on the ship.<br>\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","<br>\n","Evaluate sharpness and contrast of an image.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def evaluate_sharpness_contrast(image_path):\n","    image = cv2.imread(image_path)\n","    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    laplacian = cv2.Laplacian(gray_image, cv2.CV_64F)  # Calculate the Laplacian filter to evaluate sharpness\n","    sharpness = np.var(laplacian)  # Calculate the variance of the Laplacian image for sharpness\n","    contrast = np.max(gray_image) - np.min(gray_image)  # Calculate the contrast of the image\n","    # Normalize values between 0 and 1\n","    sharpness_normalized = sharpness / (sharpness + contrast)\n","    contrast_normalized = contrast / (sharpness + contrast)\n","    # Produce a normalized overall score\n","    score_normalized = (sharpness_normalized + contrast_normalized) / 2\n","    return score_normalized"]},{"cell_type":"markdown","metadata":{},"source":["\n","<br>\n","There are 192,225 examples; analyzing all images would be too time-consuming. Focus on a sample of 4000 images to maintain efficiency.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["contrast = set()  # Initialize a set\n","for i in range(4000):\n","    pt = image_paths[i]\n","    score = evaluate_sharpness_contrast(pt)\n","    contrast.add(score)"]},{"cell_type":"markdown","metadata":{},"source":["Display the contrast scores"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["contrast"]},{"cell_type":"markdown","metadata":{},"source":["\n","<br>\n","We notice that all images have the same contrast and sharpness score.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["shape_unique = set()\n","for i in range(4000):\n","    pt = image_paths[i]\n","    image = cv2.imread(image_paths[i])\n","    shape_unique.add(image.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Display the unique shapes of images"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["shape_unique"]},{"cell_type":"markdown","metadata":{},"source":["\n","<br>\n","Exploratory Data Analysis (EDA) provided an initial understanding of image features. Now, we will focus on specific data preprocessing techniques, including segmentation by a binary mask, to extract finer information on regions of interest. This is crucial as segmentation allows for more precise ship detection, especially in a uniform distribution of water, considering that ship positions are given in the CSV file. We will also explore model selection and hyperparameter optimization, which will be a U-Net for the moment, and don't forget about data augmentation.<br>\n"]},{"cell_type":"markdown","metadata":{},"source":["Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def rle_decode(mask_rle, shape=(768, 768)):\n","    s = mask_rle.split()\n","    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","    starts -= 1\n","    ends = starts + lengths\n","    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n","    for lo, hi in zip(starts, ends):\n","        img[lo:hi] = 1\n","    return img.reshape(shape).T"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def masks_image(in_mask_list):\n","    # Display an example of a training image and its mask\n","    all_masks = np.zeros((768, 768), dtype = np.uint8)\n","    for mask in in_mask_list:\n","        if isinstance(mask, str):\n","            all_masks |= rle_decode(mask)\n","    return all_masks"]},{"cell_type":"markdown","metadata":{},"source":["Additional data preprocessing steps"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from skimage.io import imread\n","for num in [100, 88, 5]:\n","    rle_0 = df.query(f'ImageId==\"{train_images[num-1]}\"')['EncodedPixels']\n","    img_0 = masks_image(rle_0)\n","    original = imread(train_image_dir+\"/\"+train_images[num-1])\n","    plt.figure(figsize=(15, 8))\n","    plt.subplot(1, 2, 1)\n","    plt.title(f\"Original - Train Image {original.shape}\")\n","    plt.imshow(original)\n","    plt.subplot(1, 2, 2)\n","    plt.title(f\"Mask generated from the RLE data for each ship {img_0.shape}\")\n","    plt.imshow(img_0, cmap = \"Blues_r\")\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MonData['file_size_kb']=MonData['ImageId'].map(lambda c_img_id:os.stat(os.path.join(train_image_dir,c_img_id)).st_size/1024) # file size calculation\n","MonData['file_size_kb'].hist()"]},{"cell_type":"markdown","metadata":{},"source":["Filter the data to include only entries with a file size greater than 50 KB"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MonData = MonData[MonData.file_size_kb > 50]  \n","# Plot a histogram of the 'file_size_kb' column to visualize the distribution of file sizes\n","MonData['file_size_kb'].hist()\n","# Select randomly 7 samples from the dataset\n","MonData.sample(7)  \n","# Display the shape of the dataset\n","MonData.shape"]},{"cell_type":"markdown","metadata":{},"source":["Drop the 'Size' column from the DataFrame 'df'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.drop(['Size'], axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MonData.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{},"source":["Split the 'MonData' dataset into training (train) and validation (valid) sets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train, valid = train_test_split(MonData, test_size=0.2, stratify=MonData['ships'])\n","print(\"The shape of the training dataframe is:\", train.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Merge the 'df' DataFrame with the 'train' DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_df = pd.merge(df, train)\n","train_df.sort_values(by='ImageId')  # Sort the resulting DataFrame 'train_df' by the 'ImageId' column\n","# Display the 'train_df' DataFrame\n","print(train_df)"]},{"cell_type":"markdown","metadata":{},"source":["\n","The number of rows in 'train' was 153031 after splitting into train and validation, <br>\n","but after merging, the size of the train dataframe is 184071 rows. <br>\n","This represents lines from the same image but with different ships, <br>\n"," the lines in 'df' represent ships and an image can contain multiple ships.\n"]},{"cell_type":"markdown","metadata":{},"source":["Merge with the 'df' DataFrame to create the 'valid_df' DataFrame for validation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["valid_df = pd.merge(df, valid)\n","print(\"The shape of the validation dataframe is:\", valid_df.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"We find:\")\n","print(train_df.shape[0], 'training masks,')\n","print(valid_df.shape[0], 'validation masks.')"]},{"cell_type":"markdown","metadata":{},"source":["Plot the ship count distribution in the training data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","sns.countplot(data=train_df, x='ships', palette='Set2')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["The goal of this code is to try to merge the number of ships per image into two classes <br>\n","to balance the different images"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_df['grouped_ship_count'] = train_df.ships.map(lambda x: (x+1)//2).clip(0, 7)\n","train_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["0 is the image with no ships<br>\n","1 represents images with one ship and images with two ships"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_df.grouped_ship_count.value_counts()  # Display the count of unique values in the 'grouped_ship_count' column of 'train_df'"]},{"cell_type":"markdown","metadata":{},"source":["Function to sample entries based on the number of ships"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def sample_ships(in_df, base_rep_val=2100):\n","    if in_df['ships'].values[0] == 0:  # Check if the number of ships is equal to 0\n","        return in_df.sample(base_rep_val // 3)   # Sample one-third of the value 2100\n","    else:\n","        return in_df.sample(base_rep_val)"]},{"cell_type":"markdown","metadata":{},"source":["Create a more or less balanced DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["balanced_train_df = train_df.groupby('grouped_ship_count').apply(sample_ships)\n","balanced_train_df.grouped_ship_count.value_counts()\n","print(\"The shape of the balanced dataframe is:\", balanced_train_df.shape)"]},{"cell_type":"markdown","metadata":{},"source":["For each subgroup from 0 to 7, display the number of training images grouped by the number of ships"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(8):\n","    df_val_counts = balanced_train_df[balanced_train_df.grouped_ship_count==i].ships.value_counts()\n","    print(f\"Data frame for grouped ship count = {i}:-\\n{df_val_counts}\\nSum of Values:- {df_val_counts.values.sum()}\\n\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["Visualize the distribution of training data before and after balancing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(15, 5))\n","plt.suptitle(\"Train Data\", fontsize=18, color='r', weight='bold')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.subplot(1, 2, 1)\n","sns.countplot(data=train_df, x='ships', palette='Set2')\n","plt.title(\"Ship Counts - Before Balancing\", fontsize=15)\n","plt.ylabel(\"Count\", fontsize=13)\n","plt.xlabel(\"# Ships in an image\", fontsize=13)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.subplot(1, 2, 2)\n","sns.countplot(data=balanced_train_df, x='ships', palette='Set2')\n","plt.title(\"Ship Counts - After Balancing\", fontsize=15)\n","plt.xlabel(\"# Ships in an image\", fontsize=13)\n","plt.ylabel(\"Count\", fontsize=13)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["BATCH_SIZE = 48  # Batch size\n","IMG_SCALING = (3, 3)  # Image resizing parameter"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def make_image_gen(in_df, batch_size=BATCH_SIZE):\n","    all_batches = list(in_df.groupby('ImageId'))\n","    out_rgb = []\n","    out_mask = []\n","    while True:\n","        np.random.shuffle(all_batches)  # Shuffle the order of batches\n","        for c_img_id, c_masks in all_batches:\n","            # Path to the RGB image\n","            rgb_path = os.path.join(train_image_dir, c_img_id)\n","            c_img = imread(rgb_path)  # Load the image\n","            c_mask = np.expand_dims(masks_image(c_masks['EncodedPixels'].values), -1)  # Create the mask from pixel encodings\n","            # Resize the image and mask if specified\n","            if IMG_SCALING is not None:\n","                c_img = c_img[::IMG_SCALING[0], ::IMG_SCALING[1]]\n","                c_mask = c_mask[::IMG_SCALING[0], ::IMG_SCALING[1]]\n","\n","            # Add the image and mask to the lists\n","            out_rgb += [c_img]\n","            out_mask += [c_mask]\n","            # If the batch size is reached, yield the batch\n","            if len(out_rgb) >= batch_size:\n","                yield np.stack(out_rgb, 0) / 255.0, np.stack(out_mask, 0)\n","                out_rgb, out_mask = [], []  # Reset the lists for the next batch"]},{"cell_type":"markdown","metadata":{},"source":["Call the function to generate training images"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_gen = make_image_gen(balanced_train_df)\n","# Image and Mask\n","train_x, train_y = next(train_gen)\n","print(f\"train_x ~\\nShape: {train_x.shape}\\nMin value: {train_x.min()}\\nMax value: {train_x.max()}\")\n","print(f\"\\ntrain_y ~\\nShape: {train_y.shape}\\nMin value: {train_y.min()}\\nMax value: {train_y.max()}\")"]},{"cell_type":"markdown","metadata":{},"source":["Visualize a batch of training data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from skimage.segmentation import mark_boundaries\n","montage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\n","batch_rgb = montage_rgb(train_x) \n","batch_seg = montage(train_y[:, :, :, 0])  \n","batch_overlap = mark_boundaries(batch_rgb, batch_seg.astype(int)) \n","titles = [\"Images\", \"Segmentations\", \"Ship Contours in Images\"]\n","colors = ['g', 'm', 'b']\n","display = [batch_rgb, batch_seg, batch_overlap]\n","plt.figure(figsize=(25, 10))\n","for i in range(3):\n","    plt.subplot(1, 3, i+1) \n","    plt.imshow(display[i])\n","    plt.title(titles[i], fontsize=18, color=colors[i])\n","    plt.axis('off')\n","plt.suptitle(\"Batch Visualization\", fontsize=20, color='r', weight='bold')\n","plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["VALID_IMG_COUNT = 400 # Set the number of validation images\n","valid_x, valid_y = next(make_image_gen(valid_df, VALID_IMG_COUNT))\n","print(f\"valid_x ~\\nShape: {valid_x.shape}\\nvalue min: {valid_x.min()}\\nvalue max: {valid_x.max()}\")\n","print(f\"\\nvalid_y ~\\nShape: {valid_y.shape}\\n value min: {valid_y.min()}\\nvalue max: {valid_y.max()}\")"]},{"cell_type":"markdown","metadata":{},"source":["Using ImageDataGenerator for data augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"markdown","metadata":{},"source":["Parameters for image augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dg_args = dict(rotation_range=15,          # Range of degrees for random rotations\n","               horizontal_flip=True,       # Perform random horizontal flips\n","               vertical_flip=True,         # Perform random vertical flips\n","               data_format='channels_last')  # channels_last refers to (batch, height, width, channels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["image_gen = ImageDataGenerator(**dg_args)  # Create an image generator with the specified parameters\n","label_gen = ImageDataGenerator(**dg_args)  # Create a label generator with the specified parameters"]},{"cell_type":"markdown","metadata":{},"source":["Define a function to create an augmented generator"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_aug_gen(in_gen, seed=None):\n","    np.random.seed(seed if seed is not None else np.random.choice(range(9999)))\n","    for in_x, in_y in in_gen:\n","        seed = np.random.choice(range(9999))  # Set a seed to have the same augmentation for both image and mask\n","        g_x = image_gen.flow(255 * in_x,\n","                             batch_size=in_x.shape[0],\n","                             seed=seed,\n","                             shuffle=False)\n","        g_y = label_gen.flow(in_y,\n","                             batch_size=in_x.shape[0],\n","                             seed=seed,\n","                             shuffle=False)\n","        yield next(g_x) / 255.0, next(g_y)  # Normalize the pixel values of the image and label"]},{"cell_type":"markdown","metadata":{},"source":["Augment the training data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cur_gen = create_aug_gen(train_gen, seed=42)\n","t_x, t_y = next(cur_gen)"]},{"cell_type":"markdown","metadata":{},"source":["Display information about the augmented data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\n","print('y', t_y.shape, t_y.dtype, t_y.min(), t_y.max())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import random"]},{"cell_type":"markdown","metadata":{},"source":["Total number of examples"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["total_examples = t_x.shape[0]"]},{"cell_type":"markdown","metadata":{},"source":["Set a seed to make the random selection reproducible"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["random_seed = 3\n","random.seed(random_seed)"]},{"cell_type":"markdown","metadata":{},"source":["Randomly select indices for examples to display"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["random_indices = random.sample(range(total_examples), 4)"]},{"cell_type":"markdown","metadata":{},"source":["Display randomly selected examples of images and their masks"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axes = plt.subplots(2, 4, figsize=(10, 6))\n","for i, random_index in enumerate(random_indices):\n","    # Display the image\n","    axes[0, i].imshow(t_x[random_index])\n","    axes[0, i].set_title(f\"Image {random_index + 1}\")\n","\n","    # Display the mask\n","    axes[1, i].imshow(t_y[random_index])\n","    axes[1, i].set_title(f\"Mask {random_index + 1}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Define a lambda function to create an RGB montage from an array of images"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["montage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)"]},{"cell_type":"markdown","metadata":{},"source":["Final display before passing the data to the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(25, 10))"]},{"cell_type":"markdown","metadata":{},"source":["Display the RGB montage of training images"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ax1.imshow(montage_rgb(t_x), cmap='gray')\n","ax1.set_title('Images', fontsize=18, color='g')\n","ax1.axis('off')"]},{"cell_type":"markdown","metadata":{},"source":["Display the montage of masks"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ax2.imshow(montage(t_y[:, :, :, 0]), cmap='Blues_r')\n","ax2.set_title('Masks', fontsize=18, color='r')\n","ax2.axis('off')"]},{"cell_type":"markdown","metadata":{},"source":["Display contours around ships in training images"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ax3.imshow(mark_boundaries(montage_rgb(t_x), montage(t_y[:, :, :, 0].astype(int))))\n","ax3.set_title('Bounding Box', fontsize=18, color='b')\n","ax3.axis('off')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.tight_layout()"]},{"cell_type":"markdown","metadata":{},"source":["Model Construction"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from keras import models, layers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def unet(input_size=(256, 256, 3)):\n","    inputs = layers.Input(input_size)\n","    c1 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n","    c1 = layers.Dropout(0.1)(c1)\n","    c1 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n","    p1 = layers.MaxPooling2D((2, 2))(c1)\n","    c2 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n","    c2 = layers.Dropout(0.1)(c2)\n","    c2 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n","    p2 = layers.MaxPooling2D((2, 2))(c2)\n","    c3 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n","    c3 = layers.Dropout(0.1)(c3)\n","    c3 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n","    p3 = layers.MaxPooling2D((2, 2))(c3)\n","    c4 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n","    c4 = layers.Dropout(0.1)(c4)\n","    c4 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n","    p4 = layers.MaxPooling2D((2, 2))(c4)\n","    c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n","    c5 = layers.Dropout(0.1)(c5)\n","    c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n","    u6 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n","    u6 = layers.concatenate([u6, c4])\n","    c6 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n","    c6 = layers.Dropout(0.2)(c6)\n","    c6 = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n","    u7 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n","    u7 = layers.concatenate([u7, c3])\n","    c7 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n","    c7 = layers.Dropout(0.2)(c7)\n","    c7 = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n","    u8 = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n","    u8 = layers.concatenate([u8, c2])\n","    c8 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n","    c8 = layers.Dropout(0.2)(c8)\n","    c8 = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n","    u9 = layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n","    u9 = layers.concatenate([u9, c1])\n","    c9 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n","    c9 = layers.Dropout(0.2)(c9)\n","    c9 = layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n","    out = layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n","    model = models.Model(inputs=[inputs], outputs=[out])\n","    model.summary()\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["new_model = unet()"]},{"cell_type":"markdown","metadata":{},"source":["Model layers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["new_model.layers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import keras.backend as k\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def dice_coeff(y_pred, y_true):\n","    y_true = k.cast(y_true, dtype=tf.float32)\n","    y_pred = k.cast(y_pred, dtype=tf.float32)\n","    intersection = k.sum(y_pred * y_true, axis=[1, 2, 3])\n","    union = k.sum(y_pred, axis=[1, 2, 3]) + k.sum(y_true, axis=[1, 2, 3])\n","    dice_coefficient = k.mean((2. * intersection + 1) / (union + 1))\n","    return dice_coefficient"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def focal_loss(y_pred, y_true, gamma=2, alpha=0.8):\n","    y_true = k.cast(y_true, dtype=tf.float32)\n","    y_pred = k.cast(y_pred, dtype=tf.float32)\n","    y_pred = k.flatten(y_pred)\n","    y_true = k.flatten(y_true)\n","    BCE = k.binary_crossentropy(y_pred, y_true)\n","    EXP_BCE = k.exp(-BCE)\n","    focal_loss = k.mean(alpha * k.pow((1 - EXP_BCE), gamma) * BCE)\n","    return focal_loss"]},{"cell_type":"markdown","metadata":{},"source":["Testing the function that calculates dice_coeff"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dice_coeff_result = dice_coeff(train_x, train_y)\n","print(dice_coeff_result.numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.keras.optimizers import Adam"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["new_model.compile(optimizer=Adam(1e-3, beta_1=1e-6), loss=focal_loss, metrics=[dice_coeff])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from keras.callbacks import EarlyStopping, ReduceLROnPlateau"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["reduce = ReduceLROnPlateau(monitor='val_dice_coeff', factor=0.23,\n","                           patience=3, verbose=1, mode='max',\n","                           min_delta=0.0001, cooldown=2, min_lr=1e-6)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["early = EarlyStopping(monitor=\"val_dice_coeff\", mode=\"max\", patience=20)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["callbacks_list = [reduce, early]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["aug_gen = create_aug_gen(train_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["new_model.fit(aug_gen, epochs=200, validation_data=(valid_x, valid_y), steps_per_epoch=30,\n","                  callbacks=callbacks_list)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["new_model.save('new_model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","final_model = load_model('new_model.h5', custom_objects={'focal_loss': focal_loss, 'dice_coeff': dice_coeff})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def prediction(path, img_id, model):\n","    img = imread(os.path.join(path, img_id))\n","    img = img[::3, ::3]\n","    img = img / 255.0  # Resize the image to the size expected by the model\n","    img = np.expand_dims(img, axis=0)\n","\n","    # Make sure to provide data to the predict method\n","    pred = model.predict(img)\n","    img = np.squeeze(img, axis=0)\n","    pred_squeezed = np.squeeze(pred, axis=0)\n","    return img, pred_squeezed"]},{"cell_type":"markdown","metadata":{},"source":["Model evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(cv2.imread(r\"D:\\Ship detection dataset\\test_v2\\00002bd58.jpg\").shape)\n","for sample in range(20):\n","    img, pred = prediction(r\"D:\\Ship detection dataset\\test_v2\", test_images[sample], final_model)\n","    fig = plt.figure(figsize=(8, 8))\n","    fig.add_subplot(1, 2, 1)\n","    plt.imshow(img)\n","    plt.axis('off')\n","    fig.add_subplot(1, 2, 2)\n","    plt.imshow(pred)\n","    plt.axis('off')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":2}
